# Optimization

![image](https://github.com/vassa33/alu-machine_learning/assets/61325877/cae2b276-1c6a-4cc4-8e9b-2763356b1372)

## Learning Objectives
At the end of this project, you are expected to be able to explain to anyone:
~ 
What is a hyperparameter?
How and why do you normalize your input data?
What is a saddle point?
What is stochastic gradient descent?
What is mini-batch gradient descent?
What is a moving average? How do you implement it?
What is gradient descent with momentum? How do you implement it?
What is RMSProp? How do you implement it?
What is Adam optimization? How do you implement it?
What is learning rate decay? How do you implement it?
What is batch normalization? How do you implement it? 
~
