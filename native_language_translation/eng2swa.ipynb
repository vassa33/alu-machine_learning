{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Kiswahili Translation using RNNs\n",
    "This notebook walks through the process of building a Recurrent Neural Network (RNN) to translate English text to Kiswahili. The steps included are data preprocessing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Setting Up a Virtual Environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup on Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install virtualenv if it is not installed\n",
    "!pip install virtualenv\n",
    "\n",
    "# Create a virtual environment\n",
    "!virtualenv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "!venv\\Scripts\\activate\n",
    "\n",
    "# Now, you can install dependencies within this environment\n",
    "# Note: Use `deactivate` to exit the virtual environment when you are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup on Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install virtualenv if it is not installed\n",
    "!pip install virtualenv\n",
    "\n",
    "# Create a virtual environment\n",
    "!virtualenv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "!source venv/bin/activate\n",
    "\n",
    "# Now, you can install dependencies within this environment\n",
    "# Note: Use `deactivate` to exit the virtual environment when you are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install required packages.\n",
    "# Note: This step assumes that you have already set up a Python environment.\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        lines = file.read().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines if line != \"\"]\n",
    "    return [pair[0] for pair in pairs], ['\\t' + pair[1] + '\\n' for pair in pairs]  # Add start and end tokens\n",
    "\n",
    "def tokenize(texts):\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def preprocess_data(eng_texts, swa_texts):\n",
    "    eng_tokenizer = tokenize(eng_texts)\n",
    "    swa_tokenizer = tokenize(swa_texts)\n",
    "    eng_sequences = eng_tokenizer.texts_to_sequences(eng_texts)\n",
    "    swa_sequences = swa_tokenizer.texts_to_sequences(swa_texts)\n",
    "    eng_data = pad_sequences(eng_sequences, padding='post')\n",
    "    swa_data = pad_sequences(swa_sequences, padding='post')\n",
    "    return eng_data, swa_data, eng_tokenizer, swa_tokenizer\n",
    "\n",
    "# Load and preprocess data\n",
    "train_eng_texts, train_swa_texts = load_data('data/train.txt')\n",
    "eng_data, swa_data, eng_tokenizer, swa_tokenizer = preprocess_data(train_eng_texts, train_swa_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_encoder_tokens, num_decoder_tokens, latent_dim=256):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    encoder_embedding = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_embedding(encoder_inputs))\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_embedding = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding(decoder_inputs), initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(eng_tokenizer.num_words + 1, swa_tokenizer.num_words + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training configurations\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "# Training the model\n",
    "model.fit([eng_data, swa_data[:, :-1]], np.expand_dims(swa_data[:, 1:], -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/rnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation logic, potentially using a BLEU score or similar metric\n",
    "# Placeholder for actual evaluation code\n",
    "print(\"Evaluation results: Model performs with an accuracy of X%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook guides you through the process of setting up an RNN for translating English to Kiswahili, training the model, and evaluating its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
